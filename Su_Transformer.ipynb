{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGQCj/og/bF6CaEPl7k7xI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Surya-Prasad/Transformer/blob/master/Su_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX8nkG_utLzQ",
        "outputId": "507d88b9-be8b-4bdc-864f-401f76942a99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "fatal: destination path '/content/transformer' already exists and is not an empty directory.\n",
            "/content/transformer\n",
            "Obtaining file:///content/transformer\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Checking if build backend supports build_editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build editable ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing editable metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: einops>=0.8.1 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (0.8.2)\n",
            "Requirement already satisfied: einx>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (0.3.0)\n",
            "Requirement already satisfied: jaxtyping>=0.3.0 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (0.3.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (2.0.2)\n",
            "Requirement already satisfied: psutil>=6.1.1 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (7.2.2)\n",
            "Requirement already satisfied: pytest>=8.3.4 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (8.4.2)\n",
            "Requirement already satisfied: regex>=2024.11.6 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (2025.11.3)\n",
            "Requirement already satisfied: submitit>=1.5.2 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (1.5.4)\n",
            "Requirement already satisfied: tiktoken>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (0.12.0)\n",
            "Requirement already satisfied: torch~=2.6.0 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (2.6.0)\n",
            "Requirement already satisfied: tqdm>=4.67.1 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (4.67.3)\n",
            "Requirement already satisfied: wandb>=0.19.7 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (0.25.0)\n",
            "Requirement already satisfied: ty>=0.0.1a16 in /usr/local/lib/python3.12/dist-packages (from su-transformer==1.0.6) (0.0.19)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from einx>=0.3.0->su-transformer==1.0.6) (1.13.1)\n",
            "Requirement already satisfied: frozendict in /usr/local/lib/python3.12/dist-packages (from einx>=0.3.0->su-transformer==1.0.6) (2.4.7)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping>=0.3.0->su-transformer==1.0.6) (0.1.7)\n",
            "Requirement already satisfied: iniconfig>=1 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.4->su-transformer==1.0.6) (2.3.0)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.4->su-transformer==1.0.6) (26.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.4->su-transformer==1.0.6) (1.6.0)\n",
            "Requirement already satisfied: pygments>=2.7.2 in /usr/local/lib/python3.12/dist-packages (from pytest>=8.3.4->su-transformer==1.0.6) (2.19.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.1 in /usr/local/lib/python3.12/dist-packages (from submitit>=1.5.2->su-transformer==1.0.6) (3.1.2)\n",
            "Requirement already satisfied: typing_extensions>=3.7.4.2 in /usr/local/lib/python3.12/dist-packages (from submitit>=1.5.2->su-transformer==1.0.6) (4.15.0)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken>=0.9.0->su-transformer==1.0.6) (2.32.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (3.24.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch~=2.6.0->su-transformer==1.0.6) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->einx>=0.3.0->su-transformer==1.0.6) (1.3.0)\n",
            "Requirement already satisfied: click>=8.0.1 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->su-transformer==1.0.6) (8.3.1)\n",
            "Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->su-transformer==1.0.6) (3.1.46)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->su-transformer==1.0.6) (4.9.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<7,>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->su-transformer==1.0.6) (5.29.6)\n",
            "Requirement already satisfied: pydantic<3 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->su-transformer==1.0.6) (2.12.3)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->su-transformer==1.0.6) (6.0.3)\n",
            "Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wandb>=0.19.7->su-transformer==1.0.6) (2.53.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb>=0.19.7->su-transformer==1.0.6) (4.0.12)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.19.7->su-transformer==1.0.6) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.41.4 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.19.7->su-transformer==1.0.6) (2.41.4)\n",
            "Requirement already satisfied: typing-inspection>=0.4.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3->wandb>=0.19.7->su-transformer==1.0.6) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.9.0->su-transformer==1.0.6) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.9.0->su-transformer==1.0.6) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.9.0->su-transformer==1.0.6) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken>=0.9.0->su-transformer==1.0.6) (2026.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch~=2.6.0->su-transformer==1.0.6) (3.0.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb>=0.19.7->su-transformer==1.0.6) (5.0.2)\n",
            "Building wheels for collected packages: su-transformer\n",
            "  Building editable for su-transformer (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for su-transformer: filename=su_transformer-1.0.6-py3-none-any.whl size=1362 sha256=7be017aa4c8f6afac01ebe5c45fe3bc69d4a11cbbc1570ce853d43e9f831d9dd\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-0crl2tpa/wheels/9f/50/4d/486a3e1c0f8603ad9f457b9f79a768e1ea63c797c654bf06d8\n",
            "Successfully built su-transformer\n",
            "Installing collected packages: su-transformer\n",
            "  Attempting uninstall: su-transformer\n",
            "    Found existing installation: su-transformer 1.0.6\n",
            "    Uninstalling su-transformer-1.0.6:\n",
            "      Successfully uninstalled su-transformer-1.0.6\n",
            "Successfully installed su-transformer-1.0.6\n",
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.12/dist-packages (0.12.0)\n",
            "Requirement already satisfied: jaxtyping in /usr/local/lib/python3.12/dist-packages (0.3.9)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.6.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2025.11.3)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.12/dist-packages (from tiktoken) (2.32.4)\n",
            "Requirement already satisfied: wadler-lindig>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from jaxtyping) (0.1.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.24.3)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.12/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.12/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.12/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.12/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.12/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.12/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.26.0->tiktoken) (2026.1.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# 1. Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# 2. Clone your repo\n",
        "# REPLACE THIS URL WITH YOUR ACTUAL GITHUB REPO URL!\n",
        "!git clone https://github.com/Surya-Prasad/Transformer /content/transformer\n",
        "%cd /content/transformer\n",
        "\n",
        "# 3. Create a data directory and copy the text file from your Drive\n",
        "# Adjust the path to wherever your TinyStories text file is located in your Drive\n",
        "os.makedirs(\"data\", exist_ok=True)\n",
        "!cp -r /content/drive/MyDrive/data/* data/\n",
        "\n",
        "# 4. Set up the checkpoint backup folder on Drive\n",
        "backup_dir = \"/content/drive/MyDrive/cs336_checkpoints\"\n",
        "os.makedirs(backup_dir, exist_ok=True)\n",
        "\n",
        "# 5. Install dependencies\n",
        "!pip install -e .\n",
        "!pip install tiktoken jaxtyping numpy torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prepare_custom_data.py\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os\n",
        "from cs336_basics.bpe_tokenizer import train_bpe, Tokenizer\n",
        "\n",
        "def prepare():\n",
        "    input_path = \"data/TinyStoriesV2-GPT4-train.txt\"\n",
        "    subset_path = \"data/TinyStories_subset.txt\"\n",
        "    print(\"Creating a smaller subset for BPE training...\")\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as fin:\n",
        "        with open(subset_path, \"w\", encoding=\"utf-8\") as fout:\n",
        "            for i in range(100000):\n",
        "                line = fin.readline()\n",
        "                if not line: break\n",
        "                fout.write(line)\n",
        "\n",
        "    vocab_size = 10000\n",
        "    special_tokens = [\"<|endoftext|>\"]\n",
        "\n",
        "    print(f\"Training BPE tokenizer on SUBSET (Vocab Size: {vocab_size})...\")\n",
        "    vocab, merges = train_bpe(subset_path, vocab_size, special_tokens)\n",
        "\n",
        "    with open(\"data/custom_bpe.pkl\", \"wb\") as f:\n",
        "        pickle.dump({\"vocab\": vocab, \"merges\": merges, \"special_tokens\": special_tokens}, f)\n",
        "    print(\"Saved tokenizer vocabulary to data/custom_bpe.pkl\")\n",
        "\n",
        "    print(\"Tokenizing the dataset...\")\n",
        "    tokenizer = Tokenizer(vocab, merges, special_tokens=special_tokens)\n",
        "    with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        text = f.read()\n",
        "\n",
        "    tokens = tokenizer.encode(text)\n",
        "    print(f\"Total tokens encoded: {len(tokens):,}\")\n",
        "\n",
        "    token_array = np.array(tokens, dtype=np.uint16)\n",
        "    np.save(\"data/tinystories_tokenized.npy\", token_array)\n",
        "    print(\"Saved to data/tinystories_tokenized.npy\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    prepare()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQ576M-nv61K",
        "outputId": "9bfa546e-d6b2-449f-b7ff-8bc8b238003b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prepare_custom_data.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_colab.py\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "from cs336_basics.modules import TransformerLM\n",
        "# Based on your adapters, these are imported from training\n",
        "from cs336_basics.training import AdamW, gradient_clipping, LR_cosine_schedule\n",
        "from tests.adapters import run_get_batch, run_cross_entropy, run_save_checkpoint\n",
        "\n",
        "def train():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Training on device: {device}\")\n",
        "\n",
        "    vocab_size = 10000\n",
        "    context_length = 256\n",
        "    d_model = 256\n",
        "    num_layers = 4\n",
        "    num_heads = 8\n",
        "    d_ff = 1024\n",
        "\n",
        "    batch_size = 32\n",
        "    max_iters = 5000\n",
        "    learning_rate = 5e-4\n",
        "    min_lr = 1e-5\n",
        "    warmup_iters = 100\n",
        "    lr_decay_iters = 5000\n",
        "    max_grad_norm = 1.0\n",
        "\n",
        "    print(\"Loading custom dataset...\")\n",
        "    dataset = np.load(\"data/tinystories_tokenized.npy\")\n",
        "\n",
        "    model = TransformerLM(\n",
        "        vocab_size=vocab_size, context_length=context_length,\n",
        "        d_model=d_model, num_layers=num_layers,\n",
        "        num_heads=num_heads, d_ff=d_ff, rope_theta=10000.0\n",
        "    ).to(device)\n",
        "\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate, weight_decay=0.1)\n",
        "\n",
        "    os.makedirs(\"checkpoints\", exist_ok=True)\n",
        "    backup_dir = \"/content/drive/MyDrive/cs336_checkpoints\"\n",
        "\n",
        "    t0 = time.time()\n",
        "    for it in range(max_iters+1):\n",
        "        lr = LR_cosine_schedule(it, learning_rate, min_lr, warmup_iters, lr_decay_iters)\n",
        "        for param_group in optimizer.param_groups:\n",
        "            param_group['lr'] = lr\n",
        "\n",
        "        x, y = run_get_batch(dataset, batch_size, context_length, device)\n",
        "        logits = model(x)\n",
        "        loss = run_cross_entropy(logits.view(-1, vocab_size), y.view(-1))\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        loss.backward()\n",
        "        gradient_clipping(model.parameters(), max_grad_norm)\n",
        "        optimizer.step()\n",
        "\n",
        "        if it % 10 == 0:\n",
        "            t1 = time.time()\n",
        "            dt = t1 - t0\n",
        "            t0 = t1\n",
        "            print(f\"Iter {it} | Loss: {loss.item():.4f} | LR: {lr:.2e} | Time/10-steps: {dt:.2f}s\")\n",
        "\n",
        "        if it > 0 and it % 1000 == 0:\n",
        "            ckpt_name = f\"ckpt_{it}.pt\"\n",
        "            local_path = os.path.join(\"checkpoints\", ckpt_name)\n",
        "            run_save_checkpoint(model, optimizer, it, local_path)\n",
        "\n",
        "            drive_path = os.path.join(backup_dir, ckpt_name)\n",
        "            shutil.copy(local_path, drive_path)\n",
        "            print(f\"Backed up checkpoint to Drive: {drive_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_KJDgaTrv-yD",
        "outputId": "23e19361-3bf1-422f-8068-3684d925a7bc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python prepare_custom_data.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2gpmpIBxSRL",
        "outputId": "9feb2372-9733-4a3f-a9f4-6678928bd8ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating a smaller subset for BPE training...\n",
            "Training BPE tokenizer on SUBSET (Vocab Size: 10000)...\n",
            "Saved tokenizer vocabulary to data/custom_bpe.pkl\n",
            "Tokenizing the dataset...\n",
            "Total tokens encoded: 541,787,715\n",
            "Saved to data/tinystories_tokenized.npy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_colab.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9S_wjx1Hxf-g",
        "outputId": "aa81f8a9-05c5-4aa4-a126-c5256cbe8b7c"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/transformer/tests/adapters.py:357: SyntaxWarning: invalid escape sequence '\\T'\n",
            "  rope_theta (float): The RoPE $\\Theta$ parameter.\n",
            "Training on device: cuda\n",
            "Loading custom dataset...\n",
            "Iter 0 | Loss: 9.2129 | LR: 0.00e+00 | Time/10-steps: 1.12s\n",
            "Iter 10 | Loss: 9.1287 | LR: 5.00e-05 | Time/10-steps: 2.02s\n",
            "Iter 20 | Loss: 8.8274 | LR: 1.00e-04 | Time/10-steps: 2.03s\n",
            "Iter 30 | Loss: 8.3311 | LR: 1.50e-04 | Time/10-steps: 2.03s\n",
            "Iter 40 | Loss: 7.7525 | LR: 2.00e-04 | Time/10-steps: 2.03s\n",
            "Iter 50 | Loss: 7.1115 | LR: 2.50e-04 | Time/10-steps: 2.03s\n",
            "Iter 60 | Loss: 6.4394 | LR: 3.00e-04 | Time/10-steps: 2.03s\n",
            "Iter 70 | Loss: 5.8805 | LR: 3.50e-04 | Time/10-steps: 2.04s\n",
            "Iter 80 | Loss: 5.4292 | LR: 4.00e-04 | Time/10-steps: 2.03s\n",
            "Iter 90 | Loss: 5.1634 | LR: 4.50e-04 | Time/10-steps: 2.03s\n",
            "Iter 100 | Loss: 4.8644 | LR: 5.00e-04 | Time/10-steps: 2.03s\n",
            "Iter 110 | Loss: 4.5520 | LR: 5.00e-04 | Time/10-steps: 2.03s\n",
            "Iter 120 | Loss: 4.2384 | LR: 5.00e-04 | Time/10-steps: 2.03s\n",
            "Iter 130 | Loss: 4.1145 | LR: 5.00e-04 | Time/10-steps: 2.04s\n",
            "Iter 140 | Loss: 4.1645 | LR: 5.00e-04 | Time/10-steps: 2.04s\n",
            "Iter 150 | Loss: 3.9788 | LR: 5.00e-04 | Time/10-steps: 2.04s\n",
            "Iter 160 | Loss: 3.8333 | LR: 5.00e-04 | Time/10-steps: 2.05s\n",
            "Iter 170 | Loss: 3.7195 | LR: 5.00e-04 | Time/10-steps: 2.05s\n",
            "Iter 180 | Loss: 3.9093 | LR: 5.00e-04 | Time/10-steps: 2.05s\n",
            "Iter 190 | Loss: 3.6446 | LR: 5.00e-04 | Time/10-steps: 2.05s\n",
            "Iter 200 | Loss: 3.4744 | LR: 4.99e-04 | Time/10-steps: 2.06s\n",
            "Iter 210 | Loss: 3.4991 | LR: 4.99e-04 | Time/10-steps: 2.06s\n",
            "Iter 220 | Loss: 3.5400 | LR: 4.99e-04 | Time/10-steps: 2.06s\n",
            "Iter 230 | Loss: 3.4783 | LR: 4.99e-04 | Time/10-steps: 2.07s\n",
            "Iter 240 | Loss: 3.3570 | LR: 4.99e-04 | Time/10-steps: 2.07s\n",
            "Iter 250 | Loss: 3.2926 | LR: 4.99e-04 | Time/10-steps: 2.07s\n",
            "Iter 260 | Loss: 3.2809 | LR: 4.99e-04 | Time/10-steps: 2.08s\n",
            "Iter 270 | Loss: 3.2462 | LR: 4.99e-04 | Time/10-steps: 2.08s\n",
            "Iter 280 | Loss: 3.1817 | LR: 4.98e-04 | Time/10-steps: 2.08s\n",
            "Iter 290 | Loss: 3.2294 | LR: 4.98e-04 | Time/10-steps: 2.08s\n",
            "Iter 300 | Loss: 3.1768 | LR: 4.98e-04 | Time/10-steps: 2.09s\n",
            "Iter 310 | Loss: 3.1140 | LR: 4.98e-04 | Time/10-steps: 2.09s\n",
            "Iter 320 | Loss: 3.1854 | LR: 4.98e-04 | Time/10-steps: 2.10s\n",
            "Iter 330 | Loss: 3.1688 | LR: 4.97e-04 | Time/10-steps: 2.10s\n",
            "Iter 340 | Loss: 3.0854 | LR: 4.97e-04 | Time/10-steps: 2.10s\n",
            "Iter 350 | Loss: 3.1780 | LR: 4.97e-04 | Time/10-steps: 2.10s\n",
            "Iter 360 | Loss: 3.0963 | LR: 4.97e-04 | Time/10-steps: 2.11s\n",
            "Iter 370 | Loss: 3.1300 | LR: 4.96e-04 | Time/10-steps: 2.11s\n",
            "Iter 380 | Loss: 3.1048 | LR: 4.96e-04 | Time/10-steps: 2.11s\n",
            "Iter 390 | Loss: 2.9889 | LR: 4.96e-04 | Time/10-steps: 2.12s\n",
            "Iter 400 | Loss: 2.9959 | LR: 4.95e-04 | Time/10-steps: 2.12s\n",
            "Iter 410 | Loss: 2.7874 | LR: 4.95e-04 | Time/10-steps: 2.13s\n",
            "Iter 420 | Loss: 2.8699 | LR: 4.95e-04 | Time/10-steps: 2.13s\n",
            "Iter 430 | Loss: 2.9768 | LR: 4.95e-04 | Time/10-steps: 2.13s\n",
            "Iter 440 | Loss: 2.8897 | LR: 4.94e-04 | Time/10-steps: 2.14s\n",
            "Iter 450 | Loss: 2.9087 | LR: 4.94e-04 | Time/10-steps: 2.14s\n",
            "Iter 460 | Loss: 2.8504 | LR: 4.94e-04 | Time/10-steps: 2.14s\n",
            "Iter 470 | Loss: 2.8247 | LR: 4.93e-04 | Time/10-steps: 2.15s\n",
            "Iter 480 | Loss: 2.8034 | LR: 4.93e-04 | Time/10-steps: 2.15s\n",
            "Iter 490 | Loss: 2.8286 | LR: 4.92e-04 | Time/10-steps: 2.15s\n",
            "Iter 500 | Loss: 2.6930 | LR: 4.92e-04 | Time/10-steps: 2.16s\n",
            "Iter 510 | Loss: 2.6802 | LR: 4.92e-04 | Time/10-steps: 2.16s\n",
            "Iter 520 | Loss: 2.6785 | LR: 4.91e-04 | Time/10-steps: 2.16s\n",
            "Iter 530 | Loss: 2.8958 | LR: 4.91e-04 | Time/10-steps: 2.16s\n",
            "Iter 540 | Loss: 2.8310 | LR: 4.90e-04 | Time/10-steps: 2.17s\n",
            "Iter 550 | Loss: 2.6942 | LR: 4.90e-04 | Time/10-steps: 2.18s\n",
            "Iter 560 | Loss: 2.7358 | LR: 4.89e-04 | Time/10-steps: 2.17s\n",
            "Iter 570 | Loss: 2.8196 | LR: 4.89e-04 | Time/10-steps: 2.17s\n",
            "Iter 580 | Loss: 2.7755 | LR: 4.88e-04 | Time/10-steps: 2.18s\n",
            "Iter 590 | Loss: 2.6468 | LR: 4.88e-04 | Time/10-steps: 2.17s\n",
            "Iter 600 | Loss: 2.8358 | LR: 4.88e-04 | Time/10-steps: 2.18s\n",
            "Iter 610 | Loss: 2.5516 | LR: 4.87e-04 | Time/10-steps: 2.17s\n",
            "Iter 620 | Loss: 2.7981 | LR: 4.87e-04 | Time/10-steps: 2.17s\n",
            "Iter 630 | Loss: 2.6168 | LR: 4.86e-04 | Time/10-steps: 2.17s\n",
            "Iter 640 | Loss: 2.7118 | LR: 4.85e-04 | Time/10-steps: 2.16s\n",
            "Iter 650 | Loss: 2.6927 | LR: 4.85e-04 | Time/10-steps: 2.16s\n",
            "Iter 660 | Loss: 2.7826 | LR: 4.84e-04 | Time/10-steps: 2.16s\n",
            "Iter 670 | Loss: 2.6644 | LR: 4.84e-04 | Time/10-steps: 2.15s\n",
            "Iter 680 | Loss: 2.6617 | LR: 4.83e-04 | Time/10-steps: 2.15s\n",
            "Iter 690 | Loss: 2.6454 | LR: 4.83e-04 | Time/10-steps: 2.15s\n",
            "Iter 700 | Loss: 2.6103 | LR: 4.82e-04 | Time/10-steps: 2.15s\n",
            "Iter 710 | Loss: 2.5561 | LR: 4.82e-04 | Time/10-steps: 2.15s\n",
            "Iter 720 | Loss: 2.5603 | LR: 4.81e-04 | Time/10-steps: 2.15s\n",
            "Iter 730 | Loss: 2.5735 | LR: 4.80e-04 | Time/10-steps: 2.14s\n",
            "Iter 740 | Loss: 2.7053 | LR: 4.80e-04 | Time/10-steps: 2.14s\n",
            "Iter 750 | Loss: 2.6258 | LR: 4.79e-04 | Time/10-steps: 2.14s\n",
            "Iter 760 | Loss: 2.5431 | LR: 4.78e-04 | Time/10-steps: 2.14s\n",
            "Iter 770 | Loss: 2.5296 | LR: 4.78e-04 | Time/10-steps: 2.15s\n",
            "Iter 780 | Loss: 2.5183 | LR: 4.77e-04 | Time/10-steps: 2.15s\n",
            "Iter 790 | Loss: 2.4574 | LR: 4.76e-04 | Time/10-steps: 2.15s\n",
            "Iter 800 | Loss: 2.6096 | LR: 4.76e-04 | Time/10-steps: 2.15s\n",
            "Iter 810 | Loss: 2.4868 | LR: 4.75e-04 | Time/10-steps: 2.14s\n",
            "Iter 820 | Loss: 2.5029 | LR: 4.74e-04 | Time/10-steps: 2.15s\n",
            "Iter 830 | Loss: 2.5118 | LR: 4.74e-04 | Time/10-steps: 2.15s\n",
            "Iter 840 | Loss: 2.4706 | LR: 4.73e-04 | Time/10-steps: 2.15s\n",
            "Iter 850 | Loss: 2.3569 | LR: 4.72e-04 | Time/10-steps: 2.15s\n",
            "Iter 860 | Loss: 2.4988 | LR: 4.71e-04 | Time/10-steps: 2.15s\n",
            "Iter 870 | Loss: 2.4854 | LR: 4.71e-04 | Time/10-steps: 2.15s\n",
            "Iter 880 | Loss: 2.6342 | LR: 4.70e-04 | Time/10-steps: 2.15s\n",
            "Iter 890 | Loss: 2.5222 | LR: 4.69e-04 | Time/10-steps: 2.16s\n",
            "Iter 900 | Loss: 2.4640 | LR: 4.68e-04 | Time/10-steps: 2.16s\n",
            "Iter 910 | Loss: 2.4598 | LR: 4.68e-04 | Time/10-steps: 2.16s\n",
            "Iter 920 | Loss: 2.3954 | LR: 4.67e-04 | Time/10-steps: 2.16s\n",
            "Iter 930 | Loss: 2.6247 | LR: 4.66e-04 | Time/10-steps: 2.16s\n",
            "Iter 940 | Loss: 2.4602 | LR: 4.65e-04 | Time/10-steps: 2.16s\n",
            "Iter 950 | Loss: 2.4245 | LR: 4.65e-04 | Time/10-steps: 2.16s\n",
            "Iter 960 | Loss: 2.4826 | LR: 4.64e-04 | Time/10-steps: 2.16s\n",
            "Iter 970 | Loss: 2.5184 | LR: 4.63e-04 | Time/10-steps: 2.16s\n",
            "Iter 980 | Loss: 2.3868 | LR: 4.62e-04 | Time/10-steps: 2.16s\n",
            "Iter 990 | Loss: 2.4602 | LR: 4.61e-04 | Time/10-steps: 2.16s\n",
            "Iter 1000 | Loss: 2.5038 | LR: 4.60e-04 | Time/10-steps: 2.16s\n",
            "Backed up checkpoint to Drive: /content/drive/MyDrive/cs336_checkpoints/ckpt_1000.pt\n",
            "Iter 1010 | Loss: 2.3800 | LR: 4.59e-04 | Time/10-steps: 11.08s\n",
            "Iter 1020 | Loss: 2.3760 | LR: 4.59e-04 | Time/10-steps: 2.15s\n",
            "Iter 1030 | Loss: 2.3724 | LR: 4.58e-04 | Time/10-steps: 2.15s\n",
            "Iter 1040 | Loss: 2.5042 | LR: 4.57e-04 | Time/10-steps: 2.15s\n",
            "Iter 1050 | Loss: 2.5436 | LR: 4.56e-04 | Time/10-steps: 2.15s\n",
            "Iter 1060 | Loss: 2.3126 | LR: 4.55e-04 | Time/10-steps: 2.16s\n",
            "Iter 1070 | Loss: 2.4168 | LR: 4.54e-04 | Time/10-steps: 2.17s\n",
            "Iter 1080 | Loss: 2.4877 | LR: 4.53e-04 | Time/10-steps: 2.18s\n",
            "Iter 1090 | Loss: 2.3780 | LR: 4.52e-04 | Time/10-steps: 2.18s\n",
            "Iter 1100 | Loss: 2.3426 | LR: 4.51e-04 | Time/10-steps: 2.18s\n",
            "Iter 1110 | Loss: 2.4351 | LR: 4.50e-04 | Time/10-steps: 2.18s\n",
            "Iter 1120 | Loss: 2.4897 | LR: 4.49e-04 | Time/10-steps: 2.18s\n",
            "Iter 1130 | Loss: 2.3969 | LR: 4.48e-04 | Time/10-steps: 2.18s\n",
            "Iter 1140 | Loss: 2.4151 | LR: 4.48e-04 | Time/10-steps: 2.18s\n",
            "Iter 1150 | Loss: 2.5759 | LR: 4.47e-04 | Time/10-steps: 2.17s\n",
            "Iter 1160 | Loss: 2.3863 | LR: 4.46e-04 | Time/10-steps: 2.17s\n",
            "Iter 1170 | Loss: 2.4020 | LR: 4.45e-04 | Time/10-steps: 2.16s\n",
            "Iter 1180 | Loss: 2.4255 | LR: 4.44e-04 | Time/10-steps: 2.16s\n",
            "Iter 1190 | Loss: 2.4158 | LR: 4.43e-04 | Time/10-steps: 2.17s\n",
            "Iter 1200 | Loss: 2.2896 | LR: 4.42e-04 | Time/10-steps: 2.17s\n",
            "Iter 1210 | Loss: 2.4595 | LR: 4.41e-04 | Time/10-steps: 2.16s\n",
            "Iter 1220 | Loss: 2.5967 | LR: 4.40e-04 | Time/10-steps: 2.16s\n",
            "Iter 1230 | Loss: 2.3254 | LR: 4.38e-04 | Time/10-steps: 2.16s\n",
            "Iter 1240 | Loss: 2.3888 | LR: 4.37e-04 | Time/10-steps: 2.15s\n",
            "Iter 1250 | Loss: 2.3512 | LR: 4.36e-04 | Time/10-steps: 2.16s\n",
            "Iter 1260 | Loss: 2.3031 | LR: 4.35e-04 | Time/10-steps: 2.15s\n",
            "Iter 1270 | Loss: 2.3946 | LR: 4.34e-04 | Time/10-steps: 2.15s\n",
            "Iter 1280 | Loss: 2.4933 | LR: 4.33e-04 | Time/10-steps: 2.15s\n",
            "Iter 1290 | Loss: 2.2930 | LR: 4.32e-04 | Time/10-steps: 2.15s\n",
            "Iter 1300 | Loss: 2.3236 | LR: 4.31e-04 | Time/10-steps: 2.16s\n",
            "Iter 1310 | Loss: 2.3665 | LR: 4.30e-04 | Time/10-steps: 2.16s\n",
            "Iter 1320 | Loss: 2.3311 | LR: 4.29e-04 | Time/10-steps: 2.15s\n",
            "Iter 1330 | Loss: 2.4375 | LR: 4.28e-04 | Time/10-steps: 2.15s\n",
            "Iter 1340 | Loss: 2.3678 | LR: 4.27e-04 | Time/10-steps: 2.15s\n",
            "Iter 1350 | Loss: 2.2973 | LR: 4.25e-04 | Time/10-steps: 2.16s\n",
            "Iter 1360 | Loss: 2.2331 | LR: 4.24e-04 | Time/10-steps: 2.16s\n",
            "Iter 1370 | Loss: 2.3373 | LR: 4.23e-04 | Time/10-steps: 2.16s\n",
            "Iter 1380 | Loss: 2.2941 | LR: 4.22e-04 | Time/10-steps: 2.16s\n",
            "Iter 1390 | Loss: 2.3724 | LR: 4.21e-04 | Time/10-steps: 2.15s\n",
            "Iter 1400 | Loss: 2.3079 | LR: 4.20e-04 | Time/10-steps: 2.15s\n",
            "Iter 1410 | Loss: 2.2302 | LR: 4.19e-04 | Time/10-steps: 2.16s\n",
            "Iter 1420 | Loss: 2.2825 | LR: 4.17e-04 | Time/10-steps: 2.16s\n",
            "Iter 1430 | Loss: 2.3695 | LR: 4.16e-04 | Time/10-steps: 2.16s\n",
            "Iter 1440 | Loss: 2.3860 | LR: 4.15e-04 | Time/10-steps: 2.16s\n",
            "Iter 1450 | Loss: 2.2539 | LR: 4.14e-04 | Time/10-steps: 2.16s\n",
            "Iter 1460 | Loss: 2.2659 | LR: 4.13e-04 | Time/10-steps: 2.16s\n",
            "Iter 1470 | Loss: 2.2960 | LR: 4.11e-04 | Time/10-steps: 2.16s\n",
            "Iter 1480 | Loss: 2.1495 | LR: 4.10e-04 | Time/10-steps: 2.16s\n",
            "Iter 1490 | Loss: 2.2114 | LR: 4.09e-04 | Time/10-steps: 2.16s\n",
            "Iter 1500 | Loss: 2.2140 | LR: 4.08e-04 | Time/10-steps: 2.16s\n",
            "Iter 1510 | Loss: 2.1973 | LR: 4.07e-04 | Time/10-steps: 2.16s\n",
            "Iter 1520 | Loss: 2.1807 | LR: 4.05e-04 | Time/10-steps: 2.16s\n",
            "Iter 1530 | Loss: 2.2478 | LR: 4.04e-04 | Time/10-steps: 2.16s\n",
            "Iter 1540 | Loss: 2.3184 | LR: 4.03e-04 | Time/10-steps: 2.16s\n",
            "Iter 1550 | Loss: 2.2872 | LR: 4.02e-04 | Time/10-steps: 2.16s\n",
            "Iter 1560 | Loss: 2.1681 | LR: 4.00e-04 | Time/10-steps: 2.16s\n",
            "Iter 1570 | Loss: 2.3286 | LR: 3.99e-04 | Time/10-steps: 2.17s\n",
            "Iter 1580 | Loss: 2.2698 | LR: 3.98e-04 | Time/10-steps: 2.16s\n",
            "Iter 1590 | Loss: 2.1589 | LR: 3.96e-04 | Time/10-steps: 2.16s\n",
            "Iter 1600 | Loss: 2.2354 | LR: 3.95e-04 | Time/10-steps: 2.16s\n",
            "Iter 1610 | Loss: 2.2531 | LR: 3.94e-04 | Time/10-steps: 2.16s\n",
            "Iter 1620 | Loss: 2.3969 | LR: 3.93e-04 | Time/10-steps: 2.16s\n",
            "Iter 1630 | Loss: 2.1490 | LR: 3.91e-04 | Time/10-steps: 2.17s\n",
            "Iter 1640 | Loss: 2.1857 | LR: 3.90e-04 | Time/10-steps: 2.16s\n",
            "Iter 1650 | Loss: 2.1602 | LR: 3.89e-04 | Time/10-steps: 2.16s\n",
            "Iter 1660 | Loss: 2.2326 | LR: 3.87e-04 | Time/10-steps: 2.16s\n",
            "Iter 1670 | Loss: 2.3158 | LR: 3.86e-04 | Time/10-steps: 2.16s\n",
            "Iter 1680 | Loss: 2.2439 | LR: 3.85e-04 | Time/10-steps: 2.16s\n",
            "Iter 1690 | Loss: 2.2443 | LR: 3.83e-04 | Time/10-steps: 2.16s\n",
            "Iter 1700 | Loss: 2.1407 | LR: 3.82e-04 | Time/10-steps: 2.16s\n",
            "Iter 1710 | Loss: 2.2273 | LR: 3.81e-04 | Time/10-steps: 2.16s\n",
            "Iter 1720 | Loss: 2.1815 | LR: 3.79e-04 | Time/10-steps: 2.16s\n",
            "Iter 1730 | Loss: 2.1691 | LR: 3.78e-04 | Time/10-steps: 2.16s\n",
            "Iter 1740 | Loss: 2.1940 | LR: 3.77e-04 | Time/10-steps: 2.16s\n",
            "Iter 1750 | Loss: 2.2305 | LR: 3.75e-04 | Time/10-steps: 2.16s\n",
            "Iter 1760 | Loss: 2.2224 | LR: 3.74e-04 | Time/10-steps: 2.16s\n",
            "Iter 1770 | Loss: 2.2163 | LR: 3.72e-04 | Time/10-steps: 2.16s\n",
            "Iter 1780 | Loss: 2.1224 | LR: 3.71e-04 | Time/10-steps: 2.16s\n",
            "Iter 1790 | Loss: 2.1614 | LR: 3.70e-04 | Time/10-steps: 2.16s\n",
            "Iter 1800 | Loss: 2.1624 | LR: 3.68e-04 | Time/10-steps: 2.16s\n",
            "Iter 1810 | Loss: 2.0450 | LR: 3.67e-04 | Time/10-steps: 2.16s\n",
            "Iter 1820 | Loss: 2.2405 | LR: 3.66e-04 | Time/10-steps: 2.15s\n",
            "Iter 1830 | Loss: 2.1247 | LR: 3.64e-04 | Time/10-steps: 2.16s\n",
            "Iter 1840 | Loss: 2.1585 | LR: 3.63e-04 | Time/10-steps: 2.16s\n",
            "Iter 1850 | Loss: 2.1916 | LR: 3.61e-04 | Time/10-steps: 2.16s\n",
            "Iter 1860 | Loss: 2.2575 | LR: 3.60e-04 | Time/10-steps: 2.15s\n",
            "Iter 1870 | Loss: 2.1295 | LR: 3.58e-04 | Time/10-steps: 2.16s\n",
            "Iter 1880 | Loss: 2.2684 | LR: 3.57e-04 | Time/10-steps: 2.16s\n",
            "Iter 1890 | Loss: 2.2135 | LR: 3.56e-04 | Time/10-steps: 2.16s\n",
            "Iter 1900 | Loss: 2.1900 | LR: 3.54e-04 | Time/10-steps: 2.15s\n",
            "Iter 1910 | Loss: 2.1777 | LR: 3.53e-04 | Time/10-steps: 2.15s\n",
            "Iter 1920 | Loss: 2.2254 | LR: 3.51e-04 | Time/10-steps: 2.15s\n",
            "Iter 1930 | Loss: 2.2025 | LR: 3.50e-04 | Time/10-steps: 2.15s\n",
            "Iter 1940 | Loss: 2.0604 | LR: 3.48e-04 | Time/10-steps: 2.16s\n",
            "Iter 1950 | Loss: 2.0944 | LR: 3.47e-04 | Time/10-steps: 2.16s\n",
            "Iter 1960 | Loss: 2.2824 | LR: 3.45e-04 | Time/10-steps: 2.16s\n",
            "Iter 1970 | Loss: 2.2495 | LR: 3.44e-04 | Time/10-steps: 2.16s\n",
            "Iter 1980 | Loss: 2.3420 | LR: 3.43e-04 | Time/10-steps: 2.16s\n",
            "Iter 1990 | Loss: 2.0552 | LR: 3.41e-04 | Time/10-steps: 2.16s\n",
            "Iter 2000 | Loss: 2.1888 | LR: 3.40e-04 | Time/10-steps: 2.16s\n",
            "Backed up checkpoint to Drive: /content/drive/MyDrive/cs336_checkpoints/ckpt_2000.pt\n",
            "Iter 2010 | Loss: 2.2014 | LR: 3.38e-04 | Time/10-steps: 16.00s\n",
            "Iter 2020 | Loss: 2.2005 | LR: 3.37e-04 | Time/10-steps: 2.13s\n",
            "Iter 2030 | Loss: 2.2256 | LR: 3.35e-04 | Time/10-steps: 2.14s\n",
            "Iter 2040 | Loss: 2.2456 | LR: 3.34e-04 | Time/10-steps: 2.15s\n",
            "Iter 2050 | Loss: 2.2673 | LR: 3.32e-04 | Time/10-steps: 2.15s\n",
            "Iter 2060 | Loss: 2.1340 | LR: 3.31e-04 | Time/10-steps: 2.16s\n",
            "Iter 2070 | Loss: 2.1601 | LR: 3.29e-04 | Time/10-steps: 2.16s\n",
            "Iter 2080 | Loss: 2.2586 | LR: 3.28e-04 | Time/10-steps: 2.17s\n",
            "Iter 2090 | Loss: 2.1076 | LR: 3.26e-04 | Time/10-steps: 2.18s\n",
            "Iter 2100 | Loss: 2.0864 | LR: 3.25e-04 | Time/10-steps: 2.18s\n",
            "Iter 2110 | Loss: 2.1857 | LR: 3.23e-04 | Time/10-steps: 2.18s\n",
            "Iter 2120 | Loss: 1.9924 | LR: 3.22e-04 | Time/10-steps: 2.18s\n",
            "Iter 2130 | Loss: 2.0842 | LR: 3.20e-04 | Time/10-steps: 2.19s\n",
            "Iter 2140 | Loss: 2.2895 | LR: 3.19e-04 | Time/10-steps: 2.19s\n",
            "Iter 2150 | Loss: 2.2044 | LR: 3.17e-04 | Time/10-steps: 2.19s\n",
            "Iter 2160 | Loss: 2.1617 | LR: 3.16e-04 | Time/10-steps: 2.19s\n",
            "Iter 2170 | Loss: 2.1756 | LR: 3.14e-04 | Time/10-steps: 2.18s\n",
            "Iter 2180 | Loss: 2.1377 | LR: 3.13e-04 | Time/10-steps: 2.18s\n",
            "Iter 2190 | Loss: 2.2280 | LR: 3.11e-04 | Time/10-steps: 2.18s\n",
            "Iter 2200 | Loss: 2.1391 | LR: 3.10e-04 | Time/10-steps: 2.18s\n",
            "Iter 2210 | Loss: 1.9586 | LR: 3.08e-04 | Time/10-steps: 2.18s\n",
            "Iter 2220 | Loss: 2.2122 | LR: 3.06e-04 | Time/10-steps: 2.17s\n",
            "Iter 2230 | Loss: 2.0161 | LR: 3.05e-04 | Time/10-steps: 2.17s\n",
            "Iter 2240 | Loss: 2.0463 | LR: 3.03e-04 | Time/10-steps: 2.16s\n",
            "Iter 2250 | Loss: 2.0830 | LR: 3.02e-04 | Time/10-steps: 2.16s\n",
            "Iter 2260 | Loss: 2.0865 | LR: 3.00e-04 | Time/10-steps: 2.16s\n",
            "Iter 2270 | Loss: 2.1129 | LR: 2.99e-04 | Time/10-steps: 2.16s\n",
            "Iter 2280 | Loss: 2.0610 | LR: 2.97e-04 | Time/10-steps: 2.15s\n",
            "Iter 2290 | Loss: 2.0604 | LR: 2.96e-04 | Time/10-steps: 2.15s\n",
            "Iter 2300 | Loss: 2.0948 | LR: 2.94e-04 | Time/10-steps: 2.15s\n",
            "Iter 2310 | Loss: 2.0336 | LR: 2.93e-04 | Time/10-steps: 2.15s\n",
            "Iter 2320 | Loss: 2.1610 | LR: 2.91e-04 | Time/10-steps: 2.15s\n",
            "Iter 2330 | Loss: 2.0719 | LR: 2.89e-04 | Time/10-steps: 2.15s\n",
            "Iter 2340 | Loss: 1.9444 | LR: 2.88e-04 | Time/10-steps: 2.15s\n",
            "Iter 2350 | Loss: 2.0807 | LR: 2.86e-04 | Time/10-steps: 2.15s\n",
            "Iter 2360 | Loss: 2.1320 | LR: 2.85e-04 | Time/10-steps: 2.15s\n",
            "Iter 2370 | Loss: 2.0423 | LR: 2.83e-04 | Time/10-steps: 2.15s\n",
            "Iter 2380 | Loss: 2.1052 | LR: 2.82e-04 | Time/10-steps: 2.15s\n",
            "Iter 2390 | Loss: 2.0321 | LR: 2.80e-04 | Time/10-steps: 2.15s\n",
            "Iter 2400 | Loss: 1.9549 | LR: 2.79e-04 | Time/10-steps: 2.16s\n",
            "Iter 2410 | Loss: 2.1101 | LR: 2.77e-04 | Time/10-steps: 2.15s\n",
            "Iter 2420 | Loss: 2.0647 | LR: 2.75e-04 | Time/10-steps: 2.16s\n",
            "Iter 2430 | Loss: 2.1025 | LR: 2.74e-04 | Time/10-steps: 2.15s\n",
            "Iter 2440 | Loss: 2.0560 | LR: 2.72e-04 | Time/10-steps: 2.16s\n",
            "Iter 2450 | Loss: 2.0743 | LR: 2.71e-04 | Time/10-steps: 2.16s\n",
            "Iter 2460 | Loss: 2.2218 | LR: 2.69e-04 | Time/10-steps: 2.16s\n",
            "Iter 2470 | Loss: 1.9949 | LR: 2.68e-04 | Time/10-steps: 2.16s\n",
            "Iter 2480 | Loss: 2.1308 | LR: 2.66e-04 | Time/10-steps: 2.16s\n",
            "Iter 2490 | Loss: 2.0519 | LR: 2.64e-04 | Time/10-steps: 2.16s\n",
            "Iter 2500 | Loss: 2.0189 | LR: 2.63e-04 | Time/10-steps: 2.17s\n",
            "Iter 2510 | Loss: 2.0870 | LR: 2.61e-04 | Time/10-steps: 2.16s\n",
            "Iter 2520 | Loss: 2.0283 | LR: 2.60e-04 | Time/10-steps: 2.17s\n",
            "Iter 2530 | Loss: 2.1293 | LR: 2.58e-04 | Time/10-steps: 2.16s\n",
            "Iter 2540 | Loss: 2.0918 | LR: 2.57e-04 | Time/10-steps: 2.17s\n",
            "Iter 2550 | Loss: 1.9733 | LR: 2.55e-04 | Time/10-steps: 2.16s\n",
            "Iter 2560 | Loss: 2.0412 | LR: 2.53e-04 | Time/10-steps: 2.16s\n",
            "Iter 2570 | Loss: 2.1901 | LR: 2.52e-04 | Time/10-steps: 2.16s\n",
            "Iter 2580 | Loss: 2.1288 | LR: 2.50e-04 | Time/10-steps: 2.16s\n",
            "Iter 2590 | Loss: 2.1348 | LR: 2.49e-04 | Time/10-steps: 2.16s\n",
            "Iter 2600 | Loss: 2.0665 | LR: 2.47e-04 | Time/10-steps: 2.16s\n",
            "Iter 2610 | Loss: 2.0446 | LR: 2.46e-04 | Time/10-steps: 2.16s\n",
            "Iter 2620 | Loss: 2.0981 | LR: 2.44e-04 | Time/10-steps: 2.16s\n",
            "Iter 2630 | Loss: 1.9627 | LR: 2.42e-04 | Time/10-steps: 2.16s\n",
            "Iter 2640 | Loss: 2.0212 | LR: 2.41e-04 | Time/10-steps: 2.16s\n",
            "Iter 2650 | Loss: 2.0486 | LR: 2.39e-04 | Time/10-steps: 2.16s\n",
            "Iter 2660 | Loss: 2.0648 | LR: 2.38e-04 | Time/10-steps: 2.16s\n",
            "Iter 2670 | Loss: 2.0980 | LR: 2.36e-04 | Time/10-steps: 2.16s\n",
            "Iter 2680 | Loss: 1.9792 | LR: 2.35e-04 | Time/10-steps: 2.16s\n",
            "Iter 2690 | Loss: 2.0272 | LR: 2.33e-04 | Time/10-steps: 2.16s\n",
            "Iter 2700 | Loss: 2.0317 | LR: 2.31e-04 | Time/10-steps: 2.16s\n",
            "Iter 2710 | Loss: 1.9643 | LR: 2.30e-04 | Time/10-steps: 2.16s\n",
            "Iter 2720 | Loss: 1.9841 | LR: 2.28e-04 | Time/10-steps: 2.16s\n",
            "Iter 2730 | Loss: 1.9682 | LR: 2.27e-04 | Time/10-steps: 2.16s\n",
            "Iter 2740 | Loss: 2.1483 | LR: 2.25e-04 | Time/10-steps: 2.16s\n",
            "Iter 2750 | Loss: 1.9964 | LR: 2.24e-04 | Time/10-steps: 2.16s\n",
            "Iter 2760 | Loss: 2.0258 | LR: 2.22e-04 | Time/10-steps: 2.16s\n",
            "Iter 2770 | Loss: 2.0959 | LR: 2.21e-04 | Time/10-steps: 2.16s\n",
            "Iter 2780 | Loss: 2.0513 | LR: 2.19e-04 | Time/10-steps: 2.16s\n",
            "Iter 2790 | Loss: 2.0234 | LR: 2.17e-04 | Time/10-steps: 2.16s\n",
            "Iter 2800 | Loss: 1.9569 | LR: 2.16e-04 | Time/10-steps: 2.16s\n",
            "Iter 2810 | Loss: 1.9858 | LR: 2.14e-04 | Time/10-steps: 2.16s\n",
            "Iter 2820 | Loss: 2.0565 | LR: 2.13e-04 | Time/10-steps: 2.16s\n",
            "Iter 2830 | Loss: 2.1489 | LR: 2.11e-04 | Time/10-steps: 2.16s\n",
            "Iter 2840 | Loss: 1.9708 | LR: 2.10e-04 | Time/10-steps: 2.16s\n",
            "Iter 2850 | Loss: 2.0071 | LR: 2.08e-04 | Time/10-steps: 2.16s\n",
            "Iter 2860 | Loss: 1.9717 | LR: 2.07e-04 | Time/10-steps: 2.16s\n",
            "Iter 2870 | Loss: 2.1541 | LR: 2.05e-04 | Time/10-steps: 2.16s\n",
            "Iter 2880 | Loss: 2.0120 | LR: 2.04e-04 | Time/10-steps: 2.16s\n",
            "Iter 2890 | Loss: 1.9698 | LR: 2.02e-04 | Time/10-steps: 2.16s\n",
            "Iter 2900 | Loss: 2.0038 | LR: 2.00e-04 | Time/10-steps: 2.16s\n",
            "Iter 2910 | Loss: 1.9698 | LR: 1.99e-04 | Time/10-steps: 2.16s\n",
            "Iter 2920 | Loss: 2.0282 | LR: 1.97e-04 | Time/10-steps: 2.16s\n",
            "Iter 2930 | Loss: 2.0335 | LR: 1.96e-04 | Time/10-steps: 2.16s\n",
            "Iter 2940 | Loss: 2.1023 | LR: 1.94e-04 | Time/10-steps: 2.16s\n",
            "Iter 2950 | Loss: 2.0030 | LR: 1.93e-04 | Time/10-steps: 2.16s\n",
            "Iter 2960 | Loss: 2.0542 | LR: 1.91e-04 | Time/10-steps: 2.16s\n",
            "Iter 2970 | Loss: 1.8259 | LR: 1.90e-04 | Time/10-steps: 2.16s\n",
            "Iter 2980 | Loss: 2.1668 | LR: 1.88e-04 | Time/10-steps: 2.16s\n",
            "Iter 2990 | Loss: 2.0529 | LR: 1.87e-04 | Time/10-steps: 2.16s\n",
            "Iter 3000 | Loss: 1.9860 | LR: 1.85e-04 | Time/10-steps: 2.16s\n",
            "Backed up checkpoint to Drive: /content/drive/MyDrive/cs336_checkpoints/ckpt_3000.pt\n",
            "Iter 3010 | Loss: 1.9878 | LR: 1.84e-04 | Time/10-steps: 11.23s\n",
            "Iter 3020 | Loss: 2.1150 | LR: 1.82e-04 | Time/10-steps: 2.14s\n",
            "Iter 3030 | Loss: 2.0282 | LR: 1.81e-04 | Time/10-steps: 2.15s\n",
            "Iter 3040 | Loss: 2.1272 | LR: 1.79e-04 | Time/10-steps: 2.15s\n",
            "Iter 3050 | Loss: 2.1965 | LR: 1.78e-04 | Time/10-steps: 2.16s\n",
            "Iter 3060 | Loss: 2.0868 | LR: 1.76e-04 | Time/10-steps: 2.16s\n",
            "Iter 3070 | Loss: 1.9292 | LR: 1.75e-04 | Time/10-steps: 2.16s\n",
            "Iter 3080 | Loss: 2.0655 | LR: 1.73e-04 | Time/10-steps: 2.17s\n",
            "Iter 3090 | Loss: 1.9689 | LR: 1.72e-04 | Time/10-steps: 2.18s\n",
            "Iter 3100 | Loss: 2.0213 | LR: 1.70e-04 | Time/10-steps: 2.18s\n",
            "Iter 3110 | Loss: 2.0604 | LR: 1.69e-04 | Time/10-steps: 2.18s\n",
            "Iter 3120 | Loss: 1.8785 | LR: 1.67e-04 | Time/10-steps: 2.18s\n",
            "Iter 3130 | Loss: 1.9918 | LR: 1.66e-04 | Time/10-steps: 2.18s\n",
            "Iter 3140 | Loss: 2.0098 | LR: 1.65e-04 | Time/10-steps: 2.18s\n",
            "Iter 3150 | Loss: 1.9114 | LR: 1.63e-04 | Time/10-steps: 2.18s\n",
            "Iter 3160 | Loss: 1.9783 | LR: 1.62e-04 | Time/10-steps: 2.17s\n",
            "Iter 3170 | Loss: 2.0802 | LR: 1.60e-04 | Time/10-steps: 2.17s\n",
            "Iter 3180 | Loss: 2.0860 | LR: 1.59e-04 | Time/10-steps: 2.17s\n",
            "Iter 3190 | Loss: 2.0879 | LR: 1.57e-04 | Time/10-steps: 2.17s\n",
            "Iter 3200 | Loss: 1.9639 | LR: 1.56e-04 | Time/10-steps: 2.17s\n",
            "Iter 3210 | Loss: 2.0021 | LR: 1.54e-04 | Time/10-steps: 2.16s\n",
            "Iter 3220 | Loss: 2.0369 | LR: 1.53e-04 | Time/10-steps: 2.16s\n",
            "Iter 3230 | Loss: 2.0134 | LR: 1.52e-04 | Time/10-steps: 2.16s\n",
            "Iter 3240 | Loss: 1.9518 | LR: 1.50e-04 | Time/10-steps: 2.16s\n",
            "Iter 3250 | Loss: 2.0205 | LR: 1.49e-04 | Time/10-steps: 2.16s\n",
            "Iter 3260 | Loss: 2.0704 | LR: 1.47e-04 | Time/10-steps: 2.16s\n",
            "Iter 3270 | Loss: 2.0487 | LR: 1.46e-04 | Time/10-steps: 2.15s\n",
            "Iter 3280 | Loss: 1.8709 | LR: 1.44e-04 | Time/10-steps: 2.16s\n",
            "Iter 3290 | Loss: 2.0656 | LR: 1.43e-04 | Time/10-steps: 2.15s\n",
            "Iter 3300 | Loss: 1.9277 | LR: 1.42e-04 | Time/10-steps: 2.15s\n",
            "Iter 3310 | Loss: 1.9229 | LR: 1.40e-04 | Time/10-steps: 2.16s\n",
            "Iter 3320 | Loss: 1.9215 | LR: 1.39e-04 | Time/10-steps: 2.16s\n",
            "Iter 3330 | Loss: 1.9175 | LR: 1.38e-04 | Time/10-steps: 2.15s\n",
            "Iter 3340 | Loss: 1.9784 | LR: 1.36e-04 | Time/10-steps: 2.15s\n",
            "Iter 3350 | Loss: 2.0377 | LR: 1.35e-04 | Time/10-steps: 2.15s\n",
            "Iter 3360 | Loss: 1.9678 | LR: 1.33e-04 | Time/10-steps: 2.15s\n",
            "Iter 3370 | Loss: 1.8892 | LR: 1.32e-04 | Time/10-steps: 2.16s\n",
            "Iter 3380 | Loss: 1.9848 | LR: 1.31e-04 | Time/10-steps: 2.15s\n",
            "Iter 3390 | Loss: 2.0584 | LR: 1.29e-04 | Time/10-steps: 2.16s\n",
            "Iter 3400 | Loss: 1.9565 | LR: 1.28e-04 | Time/10-steps: 2.16s\n",
            "Iter 3410 | Loss: 1.8468 | LR: 1.27e-04 | Time/10-steps: 2.16s\n",
            "Iter 3420 | Loss: 1.9713 | LR: 1.25e-04 | Time/10-steps: 2.16s\n",
            "Iter 3430 | Loss: 2.0631 | LR: 1.24e-04 | Time/10-steps: 2.16s\n",
            "Iter 3440 | Loss: 1.8904 | LR: 1.23e-04 | Time/10-steps: 2.16s\n",
            "Iter 3450 | Loss: 1.9385 | LR: 1.21e-04 | Time/10-steps: 2.16s\n",
            "Iter 3460 | Loss: 1.9554 | LR: 1.20e-04 | Time/10-steps: 2.16s\n",
            "Iter 3470 | Loss: 1.9409 | LR: 1.19e-04 | Time/10-steps: 2.16s\n",
            "Iter 3480 | Loss: 2.0798 | LR: 1.17e-04 | Time/10-steps: 2.17s\n",
            "Iter 3490 | Loss: 2.0259 | LR: 1.16e-04 | Time/10-steps: 2.16s\n",
            "Iter 3500 | Loss: 2.0452 | LR: 1.15e-04 | Time/10-steps: 2.16s\n",
            "Iter 3510 | Loss: 1.8141 | LR: 1.14e-04 | Time/10-steps: 2.16s\n",
            "Iter 3520 | Loss: 1.9095 | LR: 1.12e-04 | Time/10-steps: 2.16s\n",
            "Iter 3530 | Loss: 1.9341 | LR: 1.11e-04 | Time/10-steps: 2.16s\n",
            "Iter 3540 | Loss: 1.9337 | LR: 1.10e-04 | Time/10-steps: 2.16s\n",
            "Iter 3550 | Loss: 2.0182 | LR: 1.08e-04 | Time/10-steps: 2.16s\n",
            "Iter 3560 | Loss: 1.9830 | LR: 1.07e-04 | Time/10-steps: 2.16s\n",
            "Iter 3570 | Loss: 1.9990 | LR: 1.06e-04 | Time/10-steps: 2.16s\n",
            "Iter 3580 | Loss: 1.9161 | LR: 1.05e-04 | Time/10-steps: 2.16s\n",
            "Iter 3590 | Loss: 1.9924 | LR: 1.03e-04 | Time/10-steps: 2.16s\n",
            "Iter 3600 | Loss: 1.9636 | LR: 1.02e-04 | Time/10-steps: 2.17s\n",
            "Iter 3610 | Loss: 2.0479 | LR: 1.01e-04 | Time/10-steps: 2.16s\n",
            "Iter 3620 | Loss: 1.8396 | LR: 9.98e-05 | Time/10-steps: 2.16s\n",
            "Iter 3630 | Loss: 2.0259 | LR: 9.86e-05 | Time/10-steps: 2.16s\n",
            "Iter 3640 | Loss: 1.8821 | LR: 9.74e-05 | Time/10-steps: 2.16s\n",
            "Iter 3650 | Loss: 1.9892 | LR: 9.62e-05 | Time/10-steps: 2.16s\n",
            "Iter 3660 | Loss: 1.9834 | LR: 9.50e-05 | Time/10-steps: 2.16s\n",
            "Iter 3670 | Loss: 1.8927 | LR: 9.38e-05 | Time/10-steps: 2.16s\n",
            "Iter 3680 | Loss: 1.8914 | LR: 9.26e-05 | Time/10-steps: 2.16s\n",
            "Iter 3690 | Loss: 1.9535 | LR: 9.15e-05 | Time/10-steps: 2.16s\n",
            "Iter 3700 | Loss: 1.8179 | LR: 9.03e-05 | Time/10-steps: 2.16s\n",
            "Iter 3710 | Loss: 1.8905 | LR: 8.91e-05 | Time/10-steps: 2.16s\n",
            "Iter 3720 | Loss: 1.8618 | LR: 8.80e-05 | Time/10-steps: 2.16s\n",
            "Iter 3730 | Loss: 1.9998 | LR: 8.68e-05 | Time/10-steps: 2.16s\n",
            "Iter 3740 | Loss: 1.9593 | LR: 8.57e-05 | Time/10-steps: 2.16s\n",
            "Iter 3750 | Loss: 1.9728 | LR: 8.46e-05 | Time/10-steps: 2.16s\n",
            "Iter 3760 | Loss: 1.9968 | LR: 8.34e-05 | Time/10-steps: 2.16s\n",
            "Iter 3770 | Loss: 2.0724 | LR: 8.23e-05 | Time/10-steps: 2.16s\n",
            "Iter 3780 | Loss: 1.9093 | LR: 8.12e-05 | Time/10-steps: 2.16s\n",
            "Iter 3790 | Loss: 1.8290 | LR: 8.01e-05 | Time/10-steps: 2.16s\n",
            "Iter 3800 | Loss: 2.0785 | LR: 7.90e-05 | Time/10-steps: 2.16s\n",
            "Iter 3810 | Loss: 2.0058 | LR: 7.79e-05 | Time/10-steps: 2.16s\n",
            "Iter 3820 | Loss: 2.0682 | LR: 7.68e-05 | Time/10-steps: 2.16s\n",
            "Iter 3830 | Loss: 1.8824 | LR: 7.58e-05 | Time/10-steps: 2.16s\n",
            "Iter 3840 | Loss: 1.9486 | LR: 7.47e-05 | Time/10-steps: 2.16s\n",
            "Iter 3850 | Loss: 2.0761 | LR: 7.36e-05 | Time/10-steps: 2.16s\n",
            "Iter 3860 | Loss: 1.9261 | LR: 7.26e-05 | Time/10-steps: 2.16s\n",
            "Iter 3870 | Loss: 1.9738 | LR: 7.15e-05 | Time/10-steps: 2.16s\n",
            "Iter 3880 | Loss: 2.0041 | LR: 7.05e-05 | Time/10-steps: 2.16s\n",
            "Iter 3890 | Loss: 1.9752 | LR: 6.95e-05 | Time/10-steps: 2.16s\n",
            "Iter 3900 | Loss: 2.0297 | LR: 6.84e-05 | Time/10-steps: 2.16s\n",
            "Iter 3910 | Loss: 1.9144 | LR: 6.74e-05 | Time/10-steps: 2.16s\n",
            "Iter 3920 | Loss: 1.9633 | LR: 6.64e-05 | Time/10-steps: 2.15s\n",
            "Iter 3930 | Loss: 1.9235 | LR: 6.54e-05 | Time/10-steps: 2.16s\n",
            "Iter 3940 | Loss: 1.8825 | LR: 6.44e-05 | Time/10-steps: 2.16s\n",
            "Iter 3950 | Loss: 1.7928 | LR: 6.35e-05 | Time/10-steps: 2.16s\n",
            "Iter 3960 | Loss: 1.8840 | LR: 6.25e-05 | Time/10-steps: 2.15s\n",
            "Iter 3970 | Loss: 1.9082 | LR: 6.15e-05 | Time/10-steps: 2.16s\n",
            "Iter 3980 | Loss: 1.8729 | LR: 6.05e-05 | Time/10-steps: 2.16s\n",
            "Iter 3990 | Loss: 2.0059 | LR: 5.96e-05 | Time/10-steps: 2.15s\n",
            "Iter 4000 | Loss: 1.8849 | LR: 5.87e-05 | Time/10-steps: 2.16s\n",
            "Backed up checkpoint to Drive: /content/drive/MyDrive/cs336_checkpoints/ckpt_4000.pt\n",
            "Iter 4010 | Loss: 1.8326 | LR: 5.77e-05 | Time/10-steps: 11.82s\n",
            "Iter 4020 | Loss: 1.8892 | LR: 5.68e-05 | Time/10-steps: 2.14s\n",
            "Iter 4030 | Loss: 1.9532 | LR: 5.59e-05 | Time/10-steps: 2.14s\n",
            "Iter 4040 | Loss: 1.9037 | LR: 5.50e-05 | Time/10-steps: 2.14s\n",
            "Iter 4050 | Loss: 1.9036 | LR: 5.41e-05 | Time/10-steps: 2.15s\n",
            "Iter 4060 | Loss: 1.9301 | LR: 5.32e-05 | Time/10-steps: 2.16s\n",
            "Iter 4070 | Loss: 1.9222 | LR: 5.23e-05 | Time/10-steps: 2.17s\n",
            "Iter 4080 | Loss: 1.9606 | LR: 5.14e-05 | Time/10-steps: 2.16s\n",
            "Iter 4090 | Loss: 1.9804 | LR: 5.05e-05 | Time/10-steps: 2.17s\n",
            "Iter 4100 | Loss: 1.9698 | LR: 4.97e-05 | Time/10-steps: 2.18s\n",
            "Iter 4110 | Loss: 2.0492 | LR: 4.88e-05 | Time/10-steps: 2.18s\n",
            "Iter 4120 | Loss: 1.9080 | LR: 4.80e-05 | Time/10-steps: 2.19s\n",
            "Iter 4130 | Loss: 1.9211 | LR: 4.71e-05 | Time/10-steps: 2.19s\n",
            "Iter 4140 | Loss: 1.9909 | LR: 4.63e-05 | Time/10-steps: 2.19s\n",
            "Iter 4150 | Loss: 1.9284 | LR: 4.55e-05 | Time/10-steps: 2.18s\n",
            "Iter 4160 | Loss: 1.8797 | LR: 4.47e-05 | Time/10-steps: 2.18s\n",
            "Iter 4170 | Loss: 1.9725 | LR: 4.39e-05 | Time/10-steps: 2.18s\n",
            "Iter 4180 | Loss: 1.9606 | LR: 4.31e-05 | Time/10-steps: 2.18s\n",
            "Iter 4190 | Loss: 2.0066 | LR: 4.23e-05 | Time/10-steps: 2.17s\n",
            "Iter 4200 | Loss: 1.9570 | LR: 4.15e-05 | Time/10-steps: 2.17s\n",
            "Iter 4210 | Loss: 1.9509 | LR: 4.08e-05 | Time/10-steps: 2.17s\n",
            "Iter 4220 | Loss: 1.9770 | LR: 4.00e-05 | Time/10-steps: 2.16s\n",
            "Iter 4230 | Loss: 1.9409 | LR: 3.93e-05 | Time/10-steps: 2.16s\n",
            "Iter 4240 | Loss: 1.9981 | LR: 3.85e-05 | Time/10-steps: 2.16s\n",
            "Iter 4250 | Loss: 1.9069 | LR: 3.78e-05 | Time/10-steps: 2.16s\n",
            "Iter 4260 | Loss: 1.9876 | LR: 3.71e-05 | Time/10-steps: 2.16s\n",
            "Iter 4270 | Loss: 1.8372 | LR: 3.63e-05 | Time/10-steps: 2.16s\n",
            "Iter 4280 | Loss: 1.8899 | LR: 3.56e-05 | Time/10-steps: 2.16s\n",
            "Iter 4290 | Loss: 1.8247 | LR: 3.49e-05 | Time/10-steps: 2.15s\n",
            "Iter 4300 | Loss: 1.8554 | LR: 3.43e-05 | Time/10-steps: 2.15s\n",
            "Iter 4310 | Loss: 1.9600 | LR: 3.36e-05 | Time/10-steps: 2.15s\n",
            "Iter 4320 | Loss: 1.9324 | LR: 3.29e-05 | Time/10-steps: 2.15s\n",
            "Iter 4330 | Loss: 1.7817 | LR: 3.23e-05 | Time/10-steps: 2.15s\n",
            "Iter 4340 | Loss: 1.9256 | LR: 3.16e-05 | Time/10-steps: 2.15s\n",
            "Iter 4350 | Loss: 1.9420 | LR: 3.10e-05 | Time/10-steps: 2.15s\n",
            "Iter 4360 | Loss: 1.8470 | LR: 3.03e-05 | Time/10-steps: 2.15s\n",
            "Iter 4370 | Loss: 1.9076 | LR: 2.97e-05 | Time/10-steps: 2.15s\n",
            "Iter 4380 | Loss: 1.9172 | LR: 2.91e-05 | Time/10-steps: 2.15s\n",
            "Iter 4390 | Loss: 1.9890 | LR: 2.85e-05 | Time/10-steps: 2.15s\n",
            "Iter 4400 | Loss: 2.0526 | LR: 2.79e-05 | Time/10-steps: 2.15s\n",
            "Iter 4410 | Loss: 1.8673 | LR: 2.73e-05 | Time/10-steps: 2.15s\n",
            "Iter 4420 | Loss: 1.8649 | LR: 2.67e-05 | Time/10-steps: 2.15s\n",
            "Iter 4430 | Loss: 1.8547 | LR: 2.62e-05 | Time/10-steps: 2.15s\n",
            "Iter 4440 | Loss: 2.0065 | LR: 2.56e-05 | Time/10-steps: 2.16s\n",
            "Iter 4450 | Loss: 2.0394 | LR: 2.51e-05 | Time/10-steps: 2.16s\n",
            "Iter 4460 | Loss: 1.9303 | LR: 2.45e-05 | Time/10-steps: 2.16s\n",
            "Iter 4470 | Loss: 1.9914 | LR: 2.40e-05 | Time/10-steps: 2.16s\n",
            "Iter 4480 | Loss: 2.0253 | LR: 2.35e-05 | Time/10-steps: 2.16s\n",
            "Iter 4490 | Loss: 1.8928 | LR: 2.30e-05 | Time/10-steps: 2.16s\n",
            "Iter 4500 | Loss: 1.9260 | LR: 2.25e-05 | Time/10-steps: 2.16s\n",
            "Iter 4510 | Loss: 1.9346 | LR: 2.20e-05 | Time/10-steps: 2.16s\n",
            "Iter 4520 | Loss: 1.9301 | LR: 2.15e-05 | Time/10-steps: 2.17s\n",
            "Iter 4530 | Loss: 1.9240 | LR: 2.10e-05 | Time/10-steps: 2.17s\n",
            "Iter 4540 | Loss: 1.9319 | LR: 2.06e-05 | Time/10-steps: 2.16s\n",
            "Iter 4550 | Loss: 1.9742 | LR: 2.01e-05 | Time/10-steps: 2.17s\n",
            "Iter 4560 | Loss: 1.8421 | LR: 1.97e-05 | Time/10-steps: 2.16s\n",
            "Iter 4570 | Loss: 1.9094 | LR: 1.93e-05 | Time/10-steps: 2.16s\n",
            "Iter 4580 | Loss: 1.9524 | LR: 1.88e-05 | Time/10-steps: 2.16s\n",
            "Iter 4590 | Loss: 1.9328 | LR: 1.84e-05 | Time/10-steps: 2.16s\n",
            "Iter 4600 | Loss: 1.9212 | LR: 1.80e-05 | Time/10-steps: 2.16s\n",
            "Iter 4610 | Loss: 1.9426 | LR: 1.76e-05 | Time/10-steps: 2.16s\n",
            "Iter 4620 | Loss: 1.8958 | LR: 1.72e-05 | Time/10-steps: 2.16s\n",
            "Iter 4630 | Loss: 1.9167 | LR: 1.69e-05 | Time/10-steps: 2.16s\n",
            "Iter 4640 | Loss: 1.9153 | LR: 1.65e-05 | Time/10-steps: 2.16s\n",
            "Iter 4650 | Loss: 1.9473 | LR: 1.61e-05 | Time/10-steps: 2.16s\n",
            "Iter 4660 | Loss: 1.8832 | LR: 1.58e-05 | Time/10-steps: 2.16s\n",
            "Iter 4670 | Loss: 1.9252 | LR: 1.55e-05 | Time/10-steps: 2.16s\n",
            "Iter 4680 | Loss: 2.0669 | LR: 1.51e-05 | Time/10-steps: 2.16s\n",
            "Iter 4690 | Loss: 1.8324 | LR: 1.48e-05 | Time/10-steps: 2.16s\n",
            "Iter 4700 | Loss: 1.9383 | LR: 1.45e-05 | Time/10-steps: 2.16s\n",
            "Iter 4710 | Loss: 1.9835 | LR: 1.42e-05 | Time/10-steps: 2.16s\n",
            "Iter 4720 | Loss: 1.9545 | LR: 1.39e-05 | Time/10-steps: 2.16s\n",
            "Iter 4730 | Loss: 1.8937 | LR: 1.37e-05 | Time/10-steps: 2.15s\n",
            "Iter 4740 | Loss: 1.9155 | LR: 1.34e-05 | Time/10-steps: 2.16s\n",
            "Iter 4750 | Loss: 1.8530 | LR: 1.31e-05 | Time/10-steps: 2.16s\n",
            "Iter 4760 | Loss: 1.9280 | LR: 1.29e-05 | Time/10-steps: 2.16s\n",
            "Iter 4770 | Loss: 1.9243 | LR: 1.27e-05 | Time/10-steps: 2.16s\n",
            "Iter 4780 | Loss: 1.9561 | LR: 1.24e-05 | Time/10-steps: 2.15s\n",
            "Iter 4790 | Loss: 1.9674 | LR: 1.22e-05 | Time/10-steps: 2.16s\n",
            "Iter 4800 | Loss: 2.0028 | LR: 1.20e-05 | Time/10-steps: 2.16s\n",
            "Iter 4810 | Loss: 1.8171 | LR: 1.18e-05 | Time/10-steps: 2.16s\n",
            "Iter 4820 | Loss: 2.0203 | LR: 1.16e-05 | Time/10-steps: 2.16s\n",
            "Iter 4830 | Loss: 1.9470 | LR: 1.15e-05 | Time/10-steps: 2.16s\n",
            "Iter 4840 | Loss: 1.8802 | LR: 1.13e-05 | Time/10-steps: 2.16s\n",
            "Iter 4850 | Loss: 1.9093 | LR: 1.11e-05 | Time/10-steps: 2.15s\n",
            "Iter 4860 | Loss: 2.0258 | LR: 1.10e-05 | Time/10-steps: 2.16s\n",
            "Iter 4870 | Loss: 1.8006 | LR: 1.09e-05 | Time/10-steps: 2.16s\n",
            "Iter 4880 | Loss: 1.9268 | LR: 1.07e-05 | Time/10-steps: 2.16s\n",
            "Iter 4890 | Loss: 1.9593 | LR: 1.06e-05 | Time/10-steps: 2.16s\n",
            "Iter 4900 | Loss: 1.9226 | LR: 1.05e-05 | Time/10-steps: 2.15s\n",
            "Iter 4910 | Loss: 1.8476 | LR: 1.04e-05 | Time/10-steps: 2.16s\n",
            "Iter 4920 | Loss: 1.8423 | LR: 1.03e-05 | Time/10-steps: 2.16s\n",
            "Iter 4930 | Loss: 1.9868 | LR: 1.02e-05 | Time/10-steps: 2.16s\n",
            "Iter 4940 | Loss: 1.9662 | LR: 1.02e-05 | Time/10-steps: 2.16s\n",
            "Iter 4950 | Loss: 1.9882 | LR: 1.01e-05 | Time/10-steps: 2.16s\n",
            "Iter 4960 | Loss: 1.8749 | LR: 1.01e-05 | Time/10-steps: 2.16s\n",
            "Iter 4970 | Loss: 1.8668 | LR: 1.00e-05 | Time/10-steps: 2.16s\n",
            "Iter 4980 | Loss: 1.7861 | LR: 1.00e-05 | Time/10-steps: 2.16s\n",
            "Iter 4990 | Loss: 1.9166 | LR: 1.00e-05 | Time/10-steps: 2.16s\n",
            "Iter 5000 | Loss: 1.8230 | LR: 1.00e-05 | Time/10-steps: 2.16s\n",
            "Backed up checkpoint to Drive: /content/drive/MyDrive/cs336_checkpoints/ckpt_5000.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile generate_colab.py\n",
        "import torch\n",
        "import pickle\n",
        "from cs336_basics.modules import TransformerLM\n",
        "from cs336_basics.bpe_tokenizer import Tokenizer\n",
        "from tests.adapters import run_load_checkpoint\n",
        "\n",
        "def generate(model, idx, max_new_tokens, context_length, temperature=1.0, top_k=None, top_p=None):\n",
        "    model.eval()\n",
        "\n",
        "    for _ in range(max_new_tokens):\n",
        "        idx_cond = idx if idx.size(1) <= context_length else idx[:, -context_length:]\n",
        "\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        logits = logits[:, -1, :] / temperature\n",
        "\n",
        "        if top_k is not None:\n",
        "            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
        "            logits[logits < v[:, [-1]]] = float('-inf')\n",
        "\n",
        "        if top_p is not None:\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            cumulative_probs = torch.cumsum(torch.nn.functional.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "            sorted_indices_to_remove = cumulative_probs > top_p\n",
        "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "            sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "            indices_to_remove = sorted_indices_to_remove.scatter(dim=-1, index=sorted_indices, src=sorted_indices_to_remove)\n",
        "            logits[indices_to_remove] = float('-inf')\n",
        "\n",
        "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
        "        idx_next = torch.multinomial(probs, num_samples=1)\n",
        "        idx = torch.cat((idx, idx_next), dim=1)\n",
        "\n",
        "    return idx\n",
        "\n",
        "def main():\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "    print(f\"Generating on device: {device}\")\n",
        "\n",
        "    vocab_size = 10000\n",
        "    context_length = 256\n",
        "    d_model = 256\n",
        "    num_layers = 4\n",
        "    num_heads = 8\n",
        "    d_ff = 1024\n",
        "\n",
        "    print(\"Loading custom BPE tokenizer...\")\n",
        "    with open(\"data/custom_bpe.pkl\", \"rb\") as f:\n",
        "        bpe_data = pickle.load(f)\n",
        "\n",
        "    tokenizer = Tokenizer(\n",
        "        vocab=bpe_data[\"vocab\"],\n",
        "        merges=bpe_data[\"merges\"],\n",
        "        special_tokens=bpe_data[\"special_tokens\"]\n",
        "    )\n",
        "\n",
        "    model = TransformerLM(\n",
        "        vocab_size=vocab_size, context_length=context_length,\n",
        "        d_model=d_model, num_layers=num_layers,\n",
        "        num_heads=num_heads, d_ff=d_ff, rope_theta=10000.0\n",
        "    ).to(device)\n",
        "\n",
        "    checkpoint_path = \"checkpoints/ckpt_4000.pt\"\n",
        "    print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
        "\n",
        "    dummy_optimizer = torch.optim.AdamW(model.parameters())\n",
        "    run_load_checkpoint(checkpoint_path, model, dummy_optimizer)\n",
        "\n",
        "    prompt = \"Once upon a time, there was a little\"\n",
        "    print(f\"\\nPrompt: '{prompt}'\\n\" + \"-\"*40)\n",
        "\n",
        "    input_ids = tokenizer.encode(prompt)\n",
        "    x = torch.tensor([input_ids], dtype=torch.long).to(device)\n",
        "\n",
        "    y = generate(\n",
        "        model=model,\n",
        "        idx=x,\n",
        "        max_new_tokens=150,\n",
        "        context_length=context_length,\n",
        "        temperature=0.8,\n",
        "        top_k=None,\n",
        "        top_p=0.9\n",
        "    )\n",
        "\n",
        "    generated_text = tokenizer.decode(y[0].tolist())\n",
        "    print(generated_text)\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D7Gc_zIax4UC",
        "outputId": "3bc59962-1548-40ca-cf33-5aad4b3bc598"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing generate_colab.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python generate_colab.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I7tZLA5-3Tin",
        "outputId": "fc59c112-88ac-42d0-b794-05398597a828"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generating on device: cuda\n",
            "Loading custom BPE tokenizer...\n",
            "Loading checkpoint from checkpoints/ckpt_4000.pt...\n",
            "\n",
            "Prompt: 'Once upon a time, there was a little'\n",
            "----------------------------------------\n",
            "Once upon a time, there was a little girl named Lily. Lily loved to play outside in the sunshine. One day, she saw a new bird in the sky. The bird was very pretty and pretty. Lily wanted to meet the bird, so she put her new friend in the birdcage.\n",
            "Lily and the bird played together in the birdcage. They had so much fun! But then, something unexpected happened. The bird started to fly very fast. It flew very high and fast, up in a tree! Lily was scared at first, but she knew she could play too.\n",
            "Lily was happy and had a fun day with the bird. She said sorry to the bird and asked for help. The bird told her that the bird was just a special bird. Lily was happy and said\n",
            "----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EkxC6BjE3WM4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MwYfyq6-yyDt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}